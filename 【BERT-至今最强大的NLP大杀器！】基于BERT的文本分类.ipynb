{"cells":[{"metadata":{"_uuid":"654dc6eb422087a85df50aad9646e3c3b8692e5d","id":"09934CE1695F4EB79A565FA452E39379","mdEditEnable":false},"cell_type":"markdown","source":["#                                                                                 BERT\n","\n","BERT（Bidirectional Encoder Representations from Transformers）是一种全新的预训练的语言模型表示，发布至今也才一个多月，横扫各类NLP任务，效果卓然！实现细节及理论基础看这里：https://arxiv.org/abs/1810.04805.\n","\n","谷歌官方的Github在这里：https://github.com/google-research/bert\n","\n","BERT是一种预训练语言表示的方法，这意味着在大型文本语料库（如维基百科）上训练通用“语言理解”模型，然后将该模型用于下游的NLP任务（如问答、情感分析、文本聚类等）。 BERT优于以前的方法，因为它是第一个用于预训练NLP的无监督(Unsupervised)且深度双向（Deeply Bidirectional）的系统。"]},{"metadata":{"_uuid":"8d6fe072db4c37b40476a41be674be47312d6251","id":"648E4AD8DB954A3782489BABFE2FB008","mdEditEnable":false},"cell_type":"markdown","source":["![](https://www.lyrn.ai/wp-content/uploads/2018/11/transformer.png)"]},{"metadata":{"_uuid":"85bc6b83b518e604a855a149ff9a5ce44f991620","id":"E14A89E604EE4B048D0FE3464AB8F9EC","mdEditEnable":false},"cell_type":"markdown","source":["# 下载所有必要的依赖项\n",""]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","id":"21FE2ED4C98E48428236EB2D8FAA19C0","collapsed":false,"scrolled":false},"cell_type":"code","source":["import pandas as pd\n","import os\n","import numpy as np\n","import pandas as pd\n","import zipfile\n","from matplotlib import pyplot as plt\n","%matplotlib inline\n","import sys\n","import datetime"],"execution_count":1,"outputs":[{"output_type":"stream","text":"/opt/conda/lib/python3.5/site-packages/matplotlib/font_manager.py:229: UserWarning: Matplotlib is building the font cache using fc-list. This may take a moment.\n  'Matplotlib is building the font cache using fc-list. '\n","name":"stderr"}]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","id":"1788043FC343493CB1DE11C6DC8FB92A","collapsed":false,"scrolled":false},"cell_type":"code","source":["#下载该模型的weights和cofiguration文件\n","!wget https://storage.googleapis.com/bert_models/2018_10_18/uncased_L-12_H-768_A-12.zip"],"execution_count":2,"outputs":[{"output_type":"stream","text":"--2018-11-26 13:02:17--  https://storage.googleapis.com/bert_models/2018_10_18/uncased_L-12_H-768_A-12.zip\nResolving storage.googleapis.com (storage.googleapis.com)... 172.217.160.80, 2404:6800:4012:1::2010\nConnecting to storage.googleapis.com (storage.googleapis.com)|172.217.160.80|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 407727028 (389M) [application/zip]\nSaving to: ‘uncased_L-12_H-768_A-12.zip’\n\nuncased_L-12_H-768_ 100%[=====================>] 388.84M  8.98MB/s   in 42s    \n\n2018-11-26 13:03:04 (9.22 MB/s) - ‘uncased_L-12_H-768_A-12.zip’ saved [407727028/407727028]\n\n","name":"stdout"}]},{"metadata":{"_uuid":"8cf1f5466e25bbf042e0b6d55355d0bf7f02984e","id":"692122C0CC4341188AA759C93EB1ECA5","collapsed":false,"scrolled":false},"cell_type":"code","source":["#解压缩\n","repo = 'model_repo'\n","with zipfile.ZipFile(\"uncased_L-12_H-768_A-12.zip\",\"r\") as zip_ref:\n","    zip_ref.extractall(repo)"],"execution_count":3,"outputs":[]},{"metadata":{"_uuid":"b4a8b81c1fb8b5fae20945a56fda981550c54342","id":"F218406DD2C8484A962F8EEE970EC362","collapsed":false,"scrolled":false},"cell_type":"code","source":["!ls 'model_repo/uncased_L-12_H-768_A-12'"],"execution_count":4,"outputs":[{"output_type":"stream","text":"bert_config.json\t\t     bert_model.ckpt.index  vocab.txt\r\nbert_model.ckpt.data-00000-of-00001  bert_model.ckpt.meta\r\n","name":"stdout"}]},{"metadata":{"_uuid":"0595c54d1033b9b0e69c5e565a8063a32295c3ff","id":"AD2480CAC4194B6D9A9114DB1B22464B","collapsed":false,"scrolled":false},"cell_type":"code","source":["#下载必要的组件（py脚本）\n","!wget https://raw.githubusercontent.com/google-research/bert/master/modeling.py \n","!wget https://raw.githubusercontent.com/google-research/bert/master/optimization.py \n","!wget https://raw.githubusercontent.com/google-research/bert/master/run_classifier.py \n","!wget https://raw.githubusercontent.com/google-research/bert/master/tokenization.py "],"execution_count":5,"outputs":[{"output_type":"stream","text":"--2018-11-26 13:03:14--  https://raw.githubusercontent.com/google-research/bert/master/modeling.py\nResolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.228.133\nConnecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.228.133|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 38086 (37K) [text/plain]\nSaving to: ‘modeling.py’\n\nmodeling.py         100%[=====================>]  37.19K  19.4KB/s   in 1.9s   \n\n2018-11-26 13:03:18 (19.4 KB/s) - ‘modeling.py’ saved [38086/38086]\n\n--2018-11-26 13:03:18--  https://raw.githubusercontent.com/google-research/bert/master/optimization.py\nResolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.228.133\nConnecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.228.133|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 6046 (5.9K) [text/plain]\nSaving to: ‘optimization.py’\n\noptimization.py     100%[=====================>]   5.90K  --.-KB/s   in 0s     \n\n2018-11-26 13:03:21 (78.5 MB/s) - ‘optimization.py’ saved [6046/6046]\n\n--2018-11-26 13:03:22--  https://raw.githubusercontent.com/google-research/bert/master/run_classifier.py\nResolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.228.133\nConnecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.228.133|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 31833 (31K) [text/plain]\nSaving to: ‘run_classifier.py’\n\nrun_classifier.py   100%[=====================>]  31.09K   149KB/s   in 0.2s   \n\n2018-11-26 13:03:23 (149 KB/s) - ‘run_classifier.py’ saved [31833/31833]\n\n--2018-11-26 13:03:24--  https://raw.githubusercontent.com/google-research/bert/master/tokenization.py\nResolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.72.133\nConnecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.72.133|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 10559 (10K) [text/plain]\nSaving to: ‘tokenization.py’\n\ntokenization.py     100%[=====================>]  10.31K  55.7KB/s   in 0.2s   \n\n2018-11-26 13:03:25 (55.7 KB/s) - ‘tokenization.py’ saved [10559/10559]\n\n","name":"stdout"}]},{"metadata":{"_uuid":"68f9932a52af968d1e94d0e115fc06e3423e1d47","id":"BE5A339006DF46059E0A1345EED39DB0","mdEditEnable":false},"cell_type":"markdown","source":["下面的示例是在预处理代码上完成的，类似于** CoLa **：\n","\n","该语料库是旨在完成二进制的单句分类任务，其目标在于预测是否某个英文语句是否可以接受（是不是垃圾信息）\n","\n","可以将预训练的BERT模型用于各种任务，包括文本分类、情感分析、文本聚类。\n",""]},{"metadata":{"_uuid":"0f47047973d523029fbe9c355577133c4656ce57","id":"BF47D75CF70742C1AE13FB75D50A8B77","collapsed":false,"scrolled":false},"cell_type":"code","source":["#可获取的模型检查点（Checkpoints）:\n","#   uncased_L-12_H-768_A-12: uncased BERT base model\n","#   uncased_L-24_H-1024_A-16: uncased BERT large model\n","#   cased_L-12_H-768_A-12: cased BERT large model\n","#在这里使用基础模型\n","BERT_MODEL = 'uncased_L-12_H-768_A-12'\n","BERT_PRETRAINED_DIR = 'model_repo/uncased_L-12_H-768_A-12'\n","OUTPUT_DIR = 'model_repo/outputs'\n","print('***** Model output directory: {OUTPUT_DIR} *****')\n","print('***** BERT pretrained directory: {BERT_PRETRAINED_DIR} *****')\n",""],"execution_count":9,"outputs":[{"output_type":"stream","text":"***** Model output directory: {OUTPUT_DIR} *****\n***** BERT pretrained directory: {BERT_PRETRAINED_DIR} *****\n","name":"stdout"}]},{"metadata":{"_uuid":"3a2a0d12de6ecb82812a36549ea910b5a55c0c4a","id":"845156C2D609484480FE305131D2A914"},"cell_type":"code","source":["from sklearn.model_selection import train_test_split\n","\n","train_df =  pd.read_csv('../input/train.csv',encoding='utf-8')\n","train_df = train_df.sample(2000)\n","\n","train, test = train_test_split(train_df, test_size = 0.1, random_state=42)\n","\n","train_lines, train_labels = train.question_text.values, train.target.values\n","test_lines, test_labels = test.question_text.values, test.target.values"],"execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b8804d16e48636d8a5c1474c4384076b16c73adc","id":"2E3C25D39EFA4A248B032230495CBB65"},"cell_type":"code","source":["import modeling\n","import optimization\n","import run_classifier\n","import tokenization\n","import tensorflow as tf\n","\n","\n","def create_examples(lines, set_type, labels=None):\n","#为BERT模型构建数据\n","    guid = f'{set_type}'\n","    examples = []\n","    if guid == 'train':\n","        for line, label in zip(lines, labels):\n","            text_a = line\n","            label = str(label)\n","            examples.append(\n","              run_classifier.InputExample(guid=guid, text_a=text_a, text_b=None, label=label))\n","    else:\n","        for line in lines:\n","            text_a = line\n","            label = '0'\n","            examples.append(\n","              run_classifier.InputExample(guid=guid, text_a=text_a, text_b=None, label=label))\n","    return examples\n","\n","# 模型的超参数\n","TRAIN_BATCH_SIZE = 32\n","EVAL_BATCH_SIZE = 8\n","LEARNING_RATE = 2e-5\n","NUM_TRAIN_EPOCHS = 3.0\n","WARMUP_PROPORTION = 0.1\n","MAX_SEQ_LENGTH = 128\n","\n","# 模型配置文件\n","SAVE_CHECKPOINTS_STEPS = 1000   #如你希望在大型数据集上微调模型，请使用更大的间隔（Interval）\n","\n","# 每个checpoint的weights大概 1.5GB\n","ITERATIONS_PER_LOOP = 1000\n","NUM_TPU_CORES = 8\n","VOCAB_FILE = os.path.join(BERT_PRETRAINED_DIR, 'vocab.txt')\n","CONFIG_FILE = os.path.join(BERT_PRETRAINED_DIR, 'bert_config.json')\n","INIT_CHECKPOINT = os.path.join(BERT_PRETRAINED_DIR, 'bert_model.ckpt')\n","DO_LOWER_CASE = BERT_MODEL.startswith('uncased')\n","\n","label_list = ['0', '1']\n","tokenizer = tokenization.FullTokenizer(vocab_file=VOCAB_FILE, do_lower_case=DO_LOWER_CASE)\n","train_examples = create_examples(train_lines, 'train', labels=train_labels)\n","\n","tpu_cluster_resolver = None ##既然训练将在GPU上进行，我们不需要群集解析器\n","\n","\n","#TPUEstimator还支持CPU和GPU训练，你无需定义单独的tf.estimator.Estimator\n","run_config = tf.contrib.tpu.RunConfig(\n","    cluster=tpu_cluster_resolver,\n","    model_dir=OUTPUT_DIR,\n","    save_checkpoints_steps=SAVE_CHECKPOINTS_STEPS,\n","    tpu_config=tf.contrib.tpu.TPUConfig(\n","        iterations_per_loop=ITERATIONS_PER_LOOP,\n","        num_shards=NUM_TPU_CORES,\n","        per_host_input_for_training=tf.contrib.tpu.InputPipelineConfig.PER_HOST_V2))\n","\n","num_train_steps = int(\n","    len(train_examples) / TRAIN_BATCH_SIZE * NUM_TRAIN_EPOCHS)\n","num_warmup_steps = int(num_train_steps * WARMUP_PROPORTION)\n","\n","model_fn = run_classifier.model_fn_builder(\n","    bert_config=modeling.BertConfig.from_json_file(CONFIG_FILE),\n","    num_labels=len(label_list),\n","    init_checkpoint=INIT_CHECKPOINT,\n","    learning_rate=LEARNING_RATE,\n","    num_train_steps=num_train_steps,\n","    num_warmup_steps=num_warmup_steps,\n","    use_tpu=False, #如果设定为False，训练将在CPU或GPU上进行，具体取决于可用的设备\n","    use_one_hot_embeddings=True)\n","\n","estimator = tf.contrib.tpu.TPUEstimator(\n","    use_tpu=False, #如果设定为False，训练将在CPU或GPU上进行，具体取决于可用的设备\n","    model_fn=model_fn,\n","    config=run_config,\n","    train_batch_size=TRAIN_BATCH_SIZE,\n","    eval_batch_size=EVAL_BATCH_SIZE)"],"execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b69a1b734026d0b4b16c3eeb405e920195183873","id":"F069006C0543404B8C3AC4064F8D393B"},"cell_type":"code","source":["\"\"\"\n","Note：您可能会看到一条消息“在CPU上运行训练（'Running train on CPU'）”。\n","这实际上只意味着它运行在除了TPU之外的其他设备上，其中包括GPU。\n","\"\"\"\n","\n","# 训练模型\n","print('Please wait...')\n","train_features = run_classifier.convert_examples_to_features(\n","    train_examples, label_list, MAX_SEQ_LENGTH, tokenizer)\n","print('***** 训练起始时间 {} *****'.format(datetime.datetime.now()))\n","print('  Num examples = {}'.format(len(train_examples)))\n","print('  Batch size = {}'.format(TRAIN_BATCH_SIZE))\n","tf.logging.info(\"  Num steps = %d\", num_train_steps)\n","train_input_fn = run_classifier.input_fn_builder(\n","    features=train_features,\n","    seq_length=MAX_SEQ_LENGTH,\n","    is_training=True,\n","    drop_remainder=True)\n","estimator.train(input_fn=train_input_fn, max_steps=num_train_steps)\n","print('***** 训练结束于 {} *****'.format(datetime.datetime.now()))"],"execution_count":null,"outputs":[]},{"metadata":{"_uuid":"58fb06968c09872ccd74b44e07b4a6e932beb6df","scrolled":true,"id":"3ED51A96DE76428C8989B1387C1A6ABF"},"cell_type":"code","source":["\"\"\"\n","原始代码中有一个比较诡异的错误。\n","在预测时，估计器返回一个空的dict {}，而不使用batch_size。\n","重新定义input_fn_builder并硬编码（Hardcode）batch_size，现在正在使用'params'。\n","\"\"\"\n","\n","def input_fn_builder(features, seq_length, is_training, drop_remainder):\n","  \"\"\"创建一个`input_fn`闭包，传递给TPUEstimator。\"\"\"\n","\n","  all_input_ids = []\n","  all_input_mask = []\n","  all_segment_ids = []\n","  all_label_ids = []\n","\n","  for feature in features:\n","    all_input_ids.append(feature.input_ids)\n","    all_input_mask.append(feature.input_mask)\n","    all_segment_ids.append(feature.segment_ids)\n","    all_label_ids.append(feature.label_id)\n","\n","  def input_fn(params):\n","    \"\"\"实际的输入函数\"\"\"\n","    print(params)\n","    batch_size = 32\n","\n","    num_examples = len(features)\n","\n","    d = tf.data.Dataset.from_tensor_slices({\n","        \"input_ids\":\n","            tf.constant(\n","                all_input_ids, shape=[num_examples, seq_length],\n","                dtype=tf.int32),\n","        \"input_mask\":\n","            tf.constant(\n","                all_input_mask,\n","                shape=[num_examples, seq_length],\n","                dtype=tf.int32),\n","        \"segment_ids\":\n","            tf.constant(\n","                all_segment_ids,\n","                shape=[num_examples, seq_length],\n","                dtype=tf.int32),\n","        \"label_ids\":\n","            tf.constant(all_label_ids, shape=[num_examples], dtype=tf.int32),\n","    })\n","\n","    if is_training:\n","      d = d.repeat()\n","      d = d.shuffle(buffer_size=100)\n","\n","    d = d.batch(batch_size=batch_size, drop_remainder=drop_remainder)\n","    return d\n","\n","  return input_fn"],"execution_count":null,"outputs":[]},{"metadata":{"_uuid":"20a827baca921e1b48ac3de20422e6d384e2e450","id":"293717DC67EA43149990666E197A3BC7"},"cell_type":"code","source":["predict_examples = create_examples(test_lines, 'test')\n","\n","predict_features = run_classifier.convert_examples_to_features(\n","    predict_examples, label_list, MAX_SEQ_LENGTH, tokenizer)\n","\n","predict_input_fn = input_fn_builder(\n","    features=predict_features,\n","    seq_length=MAX_SEQ_LENGTH,\n","    is_training=False,\n","    drop_remainder=False)\n","\n","result = estimator.predict(input_fn=predict_input_fn)"],"execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ee2e686b144528d8eb5a2c50f98c52816a14c657","id":"250865EF8C464CE3927EDF62909179B0"},"cell_type":"code","source":["from tqdm import tqdm\n","preds = []\n","for prediction in tqdm(result):\n","    for class_probability in prediction:\n","      preds.append(float(class_probability))\n","\n","results = []\n","for i in tqdm(range(0,len(preds),2)):\n","  if preds[i] < 0.9:\n","    results.append(1)\n","  else:\n","    results.append(0)"],"execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4acc876dc974dd4d93525e3b163859ddfda33b2d","id":"05692D4AD3054BD681328F0CB6039DDF"},"cell_type":"code","source":["from sklearn.metrics import accuracy_score\n","from sklearn.metrics import f1_score\n","\n","print(accuracy_score(np.array(results), test_labels))\n","print(f1_score(np.array(results), test_labels))"],"execution_count":null,"outputs":[]},{"metadata":{"_uuid":"49299e2de55fc4136689cc43905fbc4690060c65","id":"C08DCE87EC4B4994B70FCCBC24F1AF01","mdEditEnable":false},"cell_type":"markdown","source":["尽管BERT效果极佳，但它目前目前仍然存在着如下缺点：\n","\n","- 训练成本昂贵。 本notebook的所有结果都在单个云TPU上进行了微调，后者具有64GB的RAM。 目前无法使用具有12GB-16GB RAM的GPU复现论文中的大多数BERT-Large运行结果，因为适配内存的最大batch size 太小。\n","\n",""]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"file_extension":".py","codemirror_mode":{"version":3,"name":"ipython"},"name":"python","nbconvert_exporter":"python","version":"3.5.5","mimetype":"text/x-python","pygments_lexer":"ipython3"}},"nbformat":4,"nbformat_minor":1}